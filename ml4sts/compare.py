# Imports: standard library
import os
import logging

# Imports: ml4sts
# Imports: first party
from ml4sts.utils import load_results, log_dataframe, save_dataframe_to_csv
from ml4sts.bootstrap import (
    compare_bootstrap_metrics,
    generate_bootstrap_distributions,
    format_bootstrap_metrics_to_dataframe,
)


def compare_across(args):
    """
    Iterate over all models in the directories given by --path_to_results.
    For each model, iterate over all directories. The first dir is baseline.
    For each directory, compare against baseline.

    Note only apples can be compared to apples; the directories you iterate over
    must have the same y_hat dimensions, and be conceptually comparable, e.g. y_hat
    arrays generated by two different models but on the same samples. Hence,
    y (actual labels) for each directory being compared must be identical.
    """
    results = load_results(args)
    results_directories = list(results["y_test_outer_folds"].keys())

    # The first directory is baseline
    baseline_dir = results_directories[0]

    # Iterate over results directories, get models that appear in every results
    # directory as a set, and cast set back to list
    model_names = set()
    for results_dir in results_directories:
        model_names_this = results["y_hat_test_outer_folds"][results_dir]
        model_names.update(set(model_names_this))
    model_names = list(model_names)

    # Get true labels from baseline directory
    y_baseline = results["y_test_outer_folds"][baseline_dir]

    # Iterate over model names
    for model_name in model_names:

        # Get labels and estimates values for baseline model
        y_hat_baseline = results["y_hat_test_outer_folds"][baseline_dir][model_name]
        y_hat_baseline_calibrated = results["y_hat_test_calibrated_outer_folds"][
            baseline_dir
        ][model_name]

        # Bootstrap sample labels and estimates for baseline directory and this model
        (
            y_bootstrap_baseline,
            y_hat_baseline_bootstrap,
            y_hat_baseline_bootstrap_calibrated,
        ) = generate_bootstrap_distributions(
            seed=args.seed,
            y=y_baseline,
            y_hat=y_hat_baseline,
            y_hat_calibrated=y_hat_baseline_calibrated,
            bootstrap_samplings=args.bootstrap_samplings,
        )

        # Iterate over results directories
        for results_dir in results_directories:

            # Isolate data in simpler variable names
            y_compare = results["y_test_outer_folds"][results_dir]
            y_hat_compare = results["y_hat_test_outer_folds"][results_dir][model_name]
            y_hat_compare_calibrated = results["y_hat_test_calibrated_outer_folds"][
                results_dir
            ][model_name]

            (
                y_bootstrap_compare,
                y_hat_compare_bootstrap,
                y_hat_compare_bootstrap_calibrated,
            ) = generate_bootstrap_distributions(
                seed=args.seed,
                y=y_compare,
                y_hat=y_hat_compare,
                y_hat_calibrated=y_hat_compare_calibrated,
                bootstrap_samplings=args.bootstrap_samplings,
            )

            metrics_bootstrap = compare_bootstrap_metrics(
                args=args,
                y_baseline=y_bootstrap_baseline,
                y_compare=y_bootstrap_compare,
                y_hat_baseline=y_hat_baseline_bootstrap,
                y_hat_baseline_calibrated=y_hat_baseline_bootstrap_calibrated,
                y_hat_compare=y_hat_compare_bootstrap,
                y_hat_compare_calibrated=y_hat_compare_bootstrap_calibrated,
                prefix_str=model_name,
                model_name_baseline=baseline_dir,
                model_name_compare=results_dir,
            )

            # Convert metrics dictionary into dataframe, and round
            df_metrics = format_bootstrap_metrics_to_dataframe(
                metrics=metrics_bootstrap,
                decimals=args.decimals,
            )

            logging.info(
                f"metrics of {model_name} comparing {baseline_dir} vs {results_dir}",
            )
            log_dataframe(
                df=df_metrics,
                format_scientific=False,
            )

            # Save metrics dataframe to CSV
            save_dataframe_to_csv(
                args=args,
                df=df_metrics,
                fname=f"metrics_compare_across_{model_name}_{baseline_dir}_vs_{results_dir}.csv",
                keep_index=True,
            )


def compare_within(args):
    """
    Iterate over all models within a single directory given by --path_to_results.
    Compare against a baseline logistic regression model.
    """
    results = load_results(args)
    results_dir = os.path.split(args.path_to_results[0])[-1]
    model_names = list(results["y_hat_test_outer_folds"][results_dir].keys())

    # Define string of baseline model
    model_name_baseline = "logreg"

    # Iterate over model names
    for model_name in model_names:

        # Isolate data in simpler variable names
        y = results["y_test_outer_folds"][results_dir]
        y_hat_compare = results["y_hat_test_outer_folds"][results_dir][model_name]
        y_hat_compare_calibrated = results["y_hat_test_calibrated_outer_folds"][
            results_dir
        ][model_name]

        y_hat_baseline = results["y_hat_test_outer_folds"][results_dir][
            model_name_baseline
        ]
        y_hat_baseline_calibrated = results["y_hat_test_calibrated_outer_folds"][
            results_dir
        ][model_name_baseline]

        # Generate bootstrap distributions: baseline
        (
            y_bootstrap_baseline,
            y_hat_baseline_bootstrap,
            y_hat_baseline_bootstrap_calibrated,
        ) = generate_bootstrap_distributions(
            seed=args.seed,
            y=y,
            y_hat=y_hat_baseline,
            y_hat_calibrated=y_hat_baseline_calibrated,
            bootstrap_samplings=args.bootstrap_samplings,
        )

        # Generate bootstrap distributions: model to compare
        (
            y_bootstrap_compare,
            y_hat_compare_bootstrap,
            y_hat_compare_bootstrap_calibrated,
        ) = generate_bootstrap_distributions(
            seed=args.seed,
            y=y,
            y_hat=y_hat_compare,
            y_hat_calibrated=y_hat_compare_calibrated,
            bootstrap_samplings=args.bootstrap_samplings,
        )

        metrics_bootstrap = compare_bootstrap_metrics(
            args=args,
            y_baseline=y_bootstrap_baseline,
            y_compare=y_bootstrap_compare,
            y_hat_baseline=y_hat_baseline_bootstrap,
            y_hat_baseline_calibrated=y_hat_baseline_bootstrap_calibrated,
            y_hat_compare=y_hat_compare_bootstrap,
            y_hat_compare_calibrated=y_hat_compare_bootstrap_calibrated,
            prefix_str="",
            model_name_baseline=model_name_baseline,
            model_name_compare=model_name,
        )

        # Convert metrics dictionary into dataframe, and round
        df_metrics = format_bootstrap_metrics_to_dataframe(
            metrics=metrics_bootstrap,
            decimals=args.decimals,
        )

        logging.info(
            f"metrics from {results_dir} comparing {model_name_baseline} vs {model_name}",
        )
        log_dataframe(
            df=df_metrics,
            format_scientific=False,
        )

        # Save metrics dataframe to CSV
        save_dataframe_to_csv(
            args=args,
            df=df_metrics,
            fname=f"metrics_compare_within_{model_name_baseline}_vs_{model_name}.csv",
            keep_index=True,
        )
